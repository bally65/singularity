# === Singularity V3 ä¸€éµè¨“ç·´ç¨‹å¼ç¢¼ (æœ€çµ‚ç›¸å®¹æ€§ä¿®å¾©ç‰ˆ) ===
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

def train_v3():
    # --- 1. æ•¸æ“šæŠ“å–èˆ‡åˆä½µ ---
    if not os.path.exists('dataset.csv'):
        print("ğŸš€ æ­£åœ¨å¾ GitHub æŠ“å–æ•¸æ“šå¡Š...")
        # Note: In Colab use !wget, in script use os.system
        os.system("wget -q https://raw.githubusercontent.com/bally65/singularity/master/dataset.csv.partaa")
        os.system("wget -q https://raw.githubusercontent.com/bally65/singularity/master/dataset.csv.partab")
        os.system("wget -q https://raw.githubusercontent.com/bally65/singularity/master/dataset.csv.partac")
        os.system("wget -q https://raw.githubusercontent.com/bally65/singularity/master/dataset.csv.partad")
        print("ğŸ“‚ æ­£åœ¨åˆä½µæ•¸æ“šå¡Š...")
        os.system("cat dataset.csv.part* > dataset.csv")
    else:
        print("ğŸ“¦ æ•¸æ“šæª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éæŠ“å–ã€‚")

    # --- 2. æ•¸æ“šè®€å–èˆ‡æ¸…ç† ---
    print("ğŸ“Š æ­£åœ¨è®€å–æ•¸æ“š (è‡ªå‹•ä¿®å¾©æ¥ç¸«éŒ¯èª¤)...")
    try:
        # ä½¿ç”¨ on_bad_lines='skip' è™•ç†åˆ†å‰²ç”¢ç”Ÿçš„æ®˜ç¼ºè¡Œ
        df = pd.read_csv('dataset.csv', on_bad_lines='skip', low_memory=False).dropna()
        print(f"âœ… æˆåŠŸè®€å– {len(df)} ç­†æœ‰æ•ˆæ•¸æ“šï¼")
    except Exception as e:
        print(f"âŒ è®€å–å¤±æ•—: {e}"); return

    # --- 3. è¶…åƒæ•¸è¨­å®š ---
    SEQ_LENGTH = 60
    BATCH_SIZE = 64
    EPOCHS = 30
    FEATURE_COLS = ['velocity', 'accel', 'entropy', 'mass', 'imbalance', 'liq_force']
    TARGET_COL = 'label_return_60s'

    # --- 4. æ¨¡å‹æ¶æ§‹: CNN + LSTM + Attention ---
    class SingularityV3Model(nn.Module):
        def __init__(self, input_dim, d_model=128, nhead=8):
            super().__init__()
            self.conv1 = nn.Conv1d(input_dim, d_model, kernel_size=3, padding=1)
            self.lstm = nn.LSTM(d_model, d_model, num_layers=2, batch_first=True, dropout=0.1)
            self.attn = nn.MultiheadAttention(d_model, nhead, dropout=0.1)
            self.head = nn.Sequential(
                nn.Linear(d_model, 64),
                nn.ReLU(),
                nn.Linear(64, 3)
            )

        def forward(self, x):
            x = x.transpose(1, 2)
            x = F.relu(self.conv1(x))
            x = x.transpose(1, 2)
            lstm_out, _ = self.lstm(x)
            attn_in = lstm_out.transpose(0, 1) # (seq, batch, dim)
            attn_out, _ = self.attn(attn_in, attn_in, attn_in)
            return self.head(attn_out[-1])

    # --- 5. æ•¸æ“šé è™•ç† (æ¨™æº–åŒ–) ---
    data = df[FEATURE_COLS].values
    labels = df[TARGET_COL].values
    data = (data - data.mean(axis=0)) / (data.std(axis=0) + 1e-6)
    
    X, Y = [], []
    for i in range(len(data) - SEQ_LENGTH):
        X.append(data[i:i+SEQ_LENGTH])
        Y.append(labels[i+SEQ_LENGTH-1])
    
    X_train, X_val, Y_train, Y_val = train_test_split(
        np.array(X), np.array(Y), test_size=0.1, shuffle=False
    )

    # --- 6. è¨“ç·´å¾ªç’° ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SingularityV3Model(len(FEATURE_COLS)).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.HuberLoss()

    print(f"ğŸ”¥ é–‹å§‹åœ¨ {device} ä¸Šè¨“ç·´ V3 æ¨¡å‹...")
    for epoch in range(EPOCHS):
        model.train()
        idx = np.random.choice(len(X_train), BATCH_SIZE)
        x_batch = torch.tensor(X_train[idx], dtype=torch.float32).to(device)
        y_batch = torch.tensor(Y_train[idx], dtype=torch.float32).to(device)
        
        optimizer.zero_grad()
        pred = model(x_batch)[:, 1]
        loss = criterion(pred, y_batch)
        loss.backward()
        optimizer.step()
        
        if (epoch+1) % 5 == 0:
            print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {loss.item():.6f}")

    # --- 7. ONNX æ¨¡å‹å°å‡º (ç›¸å®¹æ€§ä¿®å¾©å€) ---
    print("ğŸ“¦ æ­£åœ¨ä»¥ç›¸å®¹æ¨¡å¼å°å‡º ONNX æ¨¡å‹...")
    model.eval()
    model.to("cpu") # é—œéµï¼šåˆ‡æ›å› CPU å°å‡ºä»¥é¿å… FakeTensor æŒ‡é‡éŒ¯èª¤
    dummy = torch.randn(1, SEQ_LENGTH, len(FEATURE_COLS)).to("cpu")

    try:
        torch.onnx.export(
            model,
            dummy,
            "singularity_v3.onnx",
            export_params=True,
            opset_version=14, # æ”¯æ´ MultiheadAttention çš„ç©©å®šç‰ˆæœ¬
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            training=torch.onnx.TrainingMode.EVAL
        )
        print(" " + "â€”"*30)
        print("âœ… ä»»å‹™å®Œå…¨æˆåŠŸï¼")
        print("ğŸ“¦ æª”æ¡ˆ singularity_v3.onnx å·²ç”Ÿæˆï¼Œè«‹åœ¨è³‡æ–™å¤¾ä¸‹è¼‰ã€‚")
        print("â€”"*30)
    except Exception as e:
        print(f"âŒ å°å‡ºä¾ç„¶é‡åˆ°å•é¡Œ: {e}")

if __name__ == "__main__":
    train_v3()
